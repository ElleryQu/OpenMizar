To perform artifact checks, run the script `sh claims/run.sh` from the root directory. This script will download and process the dataset, and sequentially execute the functional benchmark, the safety inference benchmark, and the safety training benchmark. 

The actual test execution is carried out via the `scripts/quick_exp.py` script. This script launches multiple processes to simulate different servers, working together to complete secure computation tasks. The standard output of each process is written to `output/{test_type}/{data}-{protocol_type}`. Users can observe the execution progress of the current task in the command line, as shown in `expected/console`. Information such as task runtime and communication volume can be found in `output/{test_type}/{data}-{protocol_type}`, as demonstrated in `claims/expected/{test_type}`.